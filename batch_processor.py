from pyspark.sql import SparkSession
from pyspark.sql import functions as F
import os
import sys
import re

# --- AYARLAR ---
MINIO_ENDPOINT = os.getenv("MINIO_ENDPOINT", "http://minio:9000")
ACCESS_KEY = os.getenv("MINIO_ROOT_USER", "admin")
SECRET_KEY = os.getenv("MINIO_ROOT_PASSWORD", "admin12345")

# TÃ¼rkÃ§e karakter dÃ¼zeltme haritasÄ±
TR_CHARS = {'Ä±': 'i', 'ÄŸ': 'g', 'Ã¼': 'u', 'ÅŸ': 's', 'Ã¶': 'o', 'Ã§': 'c', 
            'Ä°': 'I', 'Äž': 'G', 'Ãœ': 'U', 'Åž': 'S', 'Ã–': 'O', 'Ã‡': 'C'}

def clean_column_name(col_name):
    """SÃ¼tun isimlerini veritabanÄ± dostu hale getirir"""
    # 1. TÃ¼rkÃ§e karakterleri deÄŸiÅŸtir
    for tr, en in TR_CHARS.items():
        col_name = col_name.replace(tr, en)
    # 2. Sadece harf, rakam ve alt Ã§izgi kalsÄ±n, gerisini sil, boÅŸluklarÄ± _ yap
    clean = re.sub(r'[^a-zA-Z0-9]', '_', col_name.strip())
    return clean.lower()

def process_batch_file(filename):
    print(f"ðŸ“¦ Dinamik Batch Ä°ÅŸlemi: {filename}")

    spark = SparkSession.builder \
        .appName("UniversalBatchProcessor") \
        .config("spark.hadoop.fs.s3a.endpoint", MINIO_ENDPOINT) \
        .config("spark.hadoop.fs.s3a.access.key", ACCESS_KEY) \
        .config("spark.hadoop.fs.s3a.secret.key", SECRET_KEY) \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .getOrCreate()

    input_path = f"s3a://market-data/raw_batch_uploads/{filename}"

    try:
        # 1. CSV'yi Oku (Otomatik Tip Tahmini ile)
        df = spark.read.option("header", "true").option("inferSchema", "true").csv(input_path)
        
        # 2. SÃ¼tun Ä°simlerini Temizle (Clean Columns)
        # "SatÄ±ÅŸ MiktarÄ± (Adet)" -> "satis_miktari_adet"
        cleaned_columns = [clean_column_name(c) for c in df.columns]
        df_clean = df.toDF(*cleaned_columns)

        # 3. Tablo Ä°smi Ãœret
        # "MÃ¼ÅŸteri Listesi 2024.csv" -> "upload_musteri_listesi_2024"
        clean_filename = clean_column_name(filename.split('.')[0])
        table_name = f"upload_{clean_filename}"

        print(f"ðŸ“Š Hedef Tablo AdÄ±: {table_name}")
        print(f"ðŸ“ SÃ¼tunlar: {cleaned_columns}")

        # 4. Veriyi Kaydet (PostgreSQL + MinIO)
        
        # A) MinIO (Parquet olarak yedekle)
        minio_path = f"s3a://market-data/batch_processed/{table_name}"
        df_clean.write.format("parquet").mode("overwrite").save(minio_path)
        print(f"âœ… MinIO'ya yedeklendi: {minio_path}")

        # B) PostgreSQL (Metabase iÃ§in)
        jdbc_url = "jdbc:postgresql://postgres:5432/market_db"
        db_properties = {
            "user": "admin",
            "password": "admin",
            "driver": "org.postgresql.Driver"
        }
        
        # mode="overwrite" -> EÄŸer aynÄ± isimde dosya daha Ã¶nce yÃ¼klendiyse tabloyu silip yeniden oluÅŸturur.
        df_clean.write.jdbc(url=jdbc_url, table=table_name, mode="overwrite", properties=db_properties)
        
        print(f"âœ… PostgreSQL'e tablo olarak yazÄ±ldÄ±: {table_name}")
        print("ðŸŽ‰ Ä°ÅŸlem TamamlandÄ±.")

    except Exception as e:
        print(f"âŒ Hata oluÅŸtu: {e}")
        sys.exit(1)
    finally:
        spark.stop()

if __name__ == "__main__":
    if len(sys.argv) > 1:
        fname = sys.argv[1]
        process_batch_file(fname)
    else:
        print("Dosya adÄ± eksik.")