import os
import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Ayarlar
MINIO_ENDPOINT = os.getenv("MINIO_ENDPOINT", "http://minio:9000")
SILVER_PATH = "s3a://market-data/silver_layer_delta"

print("ğŸ›¡ï¸ DATA QUALITY GATE (Offline Mod) BaÅŸlatÄ±lÄ±yor...")

# Spark Session - Yerel JAR'larÄ± kullanacak ÅŸekilde ayarlandÄ±
# Bu sayede her aÃ§Ä±lÄ±ÅŸta 200MB indirme yapmaz, saniyesinde aÃ§Ä±lÄ±r.
spark = SparkSession.builder \
    .appName("Quality_Guard") \
    .config("spark.jars", 
            "/opt/spark-jars/delta-core_2.12-2.4.0.jar,"
            "/opt/spark-jars/delta-storage-2.4.0.jar,"
            "/opt/spark-jars/hadoop-aws-3.3.4.jar,"
            "/opt/spark-jars/aws-java-sdk-bundle-1.12.500.jar") \
    .config("spark.driver.extraClassPath", "/opt/spark-jars/*") \
    .config("spark.executor.extraClassPath", "/opt/spark-jars/*") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .config("spark.hadoop.fs.s3a.endpoint", MINIO_ENDPOINT) \
    .config("spark.hadoop.fs.s3a.access.key", "admin") \
    .config("spark.hadoop.fs.s3a.secret.key", "admin12345") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .master("local[*]") \
    .getOrCreate()

spark.sparkContext.setLogLevel("ERROR")

try:
    # Veriyi Oku
    print(f"ğŸ“‚ Veri Okunuyor: {SILVER_PATH}")
    
    # Delta tablosu henÃ¼z oluÅŸmamÄ±ÅŸsa hata verebilir, kontrol ediyoruz
    try:
        df = spark.read.format("delta").load(SILVER_PATH)
    except Exception:
        print("âš ï¸ UYARI: Silver katmanÄ±nda henÃ¼z veri yok veya tablo oluÅŸmamÄ±ÅŸ.")
        sys.exit(0)
    
    total_rows = df.count()
    print(f"ğŸ“Š Toplam Analiz Edilen SatÄ±r: {total_rows}")

    if total_rows == 0:
        print("âš ï¸ Tablo boÅŸ, kontrol geÃ§iliyor.")
        sys.exit(0)

    # --- KURAL SETÄ° ---
    
    # Kural 1: Fiyat 0 veya daha kÃ¼Ã§Ã¼k olamaz
    bad_prices = df.filter(col("average_price") <= 0).count()
    
    # Kural 2: Volatilite hesaplanamamÄ±ÅŸ (Null) olmamalÄ±
    null_volatility = df.filter(col("volatility").isNull()).count()
    
    # Kural 3: Zaman damgasÄ± kontrolÃ¼
    null_time = df.filter(col("processed_time").isNull()).count()

    # --- RAPORLAMA ---
    print("\n" + "="*40)
    print("      KALÄ°TE KONTROL RAPORU      ")
    print("="*40)

    success = True

    if bad_prices > 0:
        print(f"âŒ [KRÄ°TÄ°K] Negatif/SÄ±fÄ±r Fiyat HatasÄ±: {bad_prices} kayÄ±t")
        success = False
    else:
        print("âœ… Fiyat KontrolÃ¼: BAÅARILI")

    if null_volatility > 0:
        print(f"âš ï¸ [UYARI] Eksik Volatilite Verisi: {null_volatility} kayÄ±t")
    else:
        print("âœ… Volatilite KontrolÃ¼: BAÅARILI")
        
    if null_time > 0:
        print(f"âŒ [KRÄ°TÄ°K] Zaman DamgasÄ± HatasÄ±: {null_time} kayÄ±t")
        success = False
    else:
        print("âœ… Zaman DamgasÄ± KontrolÃ¼: BAÅARILI")

    print("-" * 40)
    
    if success:
        print("ğŸ‰ SONUÃ‡: VERÄ° KALÄ°TESÄ° MÃœKEMMEL (PASSED)")
    else:
        print("ğŸš« SONUÃ‡: VERÄ°DE HATALAR VAR (FAILED)")

except Exception as e:
    print(f"âš ï¸ Kritik Sistem HatasÄ±: {e}")

spark.stop()