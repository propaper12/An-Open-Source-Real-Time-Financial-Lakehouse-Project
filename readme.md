<div align="center"> <img src="[https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white](https://www.google.com/search?q=https://img.shields.io/badge/Python-3776AB%3Fstyle%3Dfor-the-badge%26logo%3Dpython%26logoColor%3Dwhite)" /> <img src="[https://img.shields.io/badge/Apache_Spark-E25A1C?style=for-the-badge&logo=apache-spark&logoColor=white](https://www.google.com/search?q=https://img.shields.io/badge/Apache_Spark-E25A1C%3Fstyle%3Dfor-the-badge%26logo%3Dapache-spark%26logoColor%3Dwhite)" /> <img src="[https://img.shields.io/badge/Apache_Kafka-231F20?style=for-the-badge&logo=apache-kafka&logoColor=white](https://img.shields.io/badge/Apache_Kafka-231F20?style=for-the-badge&logo=apache-kafka&logoColor=white)" /> <img src="[https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white](https://www.google.com/search?q=https://img.shields.io/badge/Docker-2496ED%3Fstyle%3Dfor-the-badge%26logo%3Ddocker%26logoColor%3Dwhite)" /> <img src="[https://img.shields.io/badge/PostgreSQL-4169E1?style=for-the-badge&logo=postgresql&logoColor=white](https://www.google.com/search?q=https://img.shields.io/badge/PostgreSQL-4169E1%3Fstyle%3Dfor-the-badge%26logo%3Dpostgresql%26logoColor%3Dwhite)" /> </div>

# Enterprise Real-Time Lakehouse & MLOps Platform

Bu platform, Binance WebSocket ve Ã¶zel API kanallarÄ± Ã¼zerinden elde edilen gerÃ§ek zamanlÄ± piyasa verilerini yÃ¼ksek performanslÄ± veri iÅŸleme altyapÄ±sÄ± ile analiz eden, Delta Lake mimarisi Ã¼zerinde gÃ¼venli ve Ã¶lÃ§eklenebilir biÃ§imde depolayan, Apache Spark MLlib ile geliÅŸmiÅŸ makine Ã¶ÄŸrenmesi modelleri eÄŸiten ve dbt aracÄ±lÄ±ÄŸÄ±yla kurumsal seviyede analitik katmanlar oluÅŸturan uÃ§tan uca bir veri mÃ¼hendisliÄŸi ve MLOps platformudur.

Sistem mimarisi, verinin alÄ±nmasÄ±ndan, iÅŸlenmesine, depolanmasÄ±na, panellere aktarÄ±lmasÄ±na ve yapay zeka tabanlÄ± tahminlerin Ã¼retilmesine kadar olan tÃ¼m sÃ¼reci ortalama 5 saniye iÃ§erisinde tamamlayacak ÅŸekilde gerÃ§ek zamanlÄ± olarak tasarlanmÄ±ÅŸtÄ±r. Bu sayede kullanÄ±cÄ±lar, minimum gecikme ile yÃ¼ksek doÄŸruluklu analiz ve tahminlere eriÅŸebilmektedir.

Platform, yÃ¼ksek maliyetli ticari kripto analiz panellerine alternatif olarak, aÃ§Ä±k kaynak felsefesiyle geliÅŸtirilmiÅŸtir. Bu sayede bireysel kullanÄ±cÄ±lar, araÅŸtÄ±rmacÄ±lar ve ÅŸirketler, sistemi Ã¶zelleÅŸtirilebilir, Ã¶lÃ§eklenebilir ve sÃ¼rdÃ¼rÃ¼lebilir bir ÅŸekilde kendi ihtiyaÃ§larÄ±na gÃ¶re yapÄ±landÄ±rabilir.

Ã–zellikle kodlama yetkinliÄŸine sahip kripto yatÄ±rÄ±mcÄ±larÄ± ve veri mÃ¼hendisleri, bu altyapÄ±yÄ± kendi stratejileri doÄŸrultusunda geliÅŸtirerek, Ã¶zel gÃ¶sterge panelleri, ileri seviye analiz modÃ¼lleri ve otomatik alÄ±m satÄ±m stratejileri oluÅŸturabilir. Kurumsal tarafta ise ÅŸirketler, platformu kendi veri kaynaklarÄ±yla entegre ederek kurum iÃ§i, yÃ¼ksek performanslÄ± ve gÃ¼venli bir analitik ekosistem inÅŸa edebilir.

Bu mimari, gerÃ§ek zamanlÄ± veri akÄ±ÅŸÄ±, bÃ¼yÃ¼k veri iÅŸleme, geliÅŸmiÅŸ makine Ã¶ÄŸrenmesi ve profesyonel veri modelleme sÃ¼reÃ§lerini tek Ã§atÄ± altÄ±nda birleÅŸtirerek, modern veri platformlarÄ± iÃ§in gÃ¼Ã§lÃ¼ ve sÃ¼rdÃ¼rÃ¼lebilir bir referans mimari sunar.

<img width="2816" height="1536" alt="Gemini_Generated_Image_ywap46ywap46ywap" src="https://github.com/user-attachments/assets/0d3cabf3-f35d-4d77-ad85-a01477a16265" />
----------
## ğŸ“‚ Proje YapÄ±sÄ±

```text
.
â”œâ”€â”€ dags/                       # Airflow Ä°ÅŸ AkÄ±ÅŸlarÄ± (DAGs)(ArtÄ±k projeden kaldrÄ±ldÄ± ama denemek Ä±steyenelr iÃ§in silinmedi duruyor ama bir islevi yok artÄ±k)
â”‚   â”œâ”€â”€ dbt_dag.py
â”‚   â”œâ”€â”€ mlops_pipeline.py
â”‚   â”œâ”€â”€ otomatik_egitim.py
â”‚   â””â”€â”€ spark_ml_pipeline.py
â”‚
â”œâ”€â”€ dashboard_app/              # Streamlit KullanÄ±cÄ± ArayÃ¼zÃ¼
â”‚   â”œâ”€â”€ Home.py                 # Ana Sayfa
â”‚   â”œâ”€â”€ utils.py
â”‚   â”œâ”€â”€ admin_modules/          # YÃ¶netim ve Backend Ä°ÅŸlemleri
â”‚   â””â”€â”€ pages/                  # Uygulama SayfalarÄ± (CanlÄ± Piyasa, MLOps vb.)
â”‚
â”œâ”€â”€ dbt_project/                # Veri DÃ¶nÃ¼ÅŸÃ¼m KatmanÄ± (DBT)
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ profiles.yml
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ models/                 # SQL Modelleri (Staging & Marts)
â”‚  â””â”€â”€â”€target/  
â”œâ”€â”€ Dokumanlar/                 # Proje DokÃ¼mantasyonu
â”œâ”€â”€25.3                         #kullancÄ±nÄ±n projemle alakalÄ± herseye ualsacagÄ± genel yazÄ±larÄ±m
â”œâ”€â”€ batch_processor.py          # Toplu veri iÅŸleme servisi
â”œâ”€â”€ Dokumanlar/                 # Proje DokÃ¼mantasyonu
â”œâ”€â”€ .env                        # Ã‡evresel DeÄŸiÅŸkenler (Åifreler - Git'e atÄ±lmaz)
â”œâ”€â”€ .gitignore                  # Git
â”œâ”€â”€ batch_processor.py          # GeÃ§miÅŸ veri (CSV) toplu iÅŸleme servisi
â”œâ”€â”€ batch_user_processor.py     # KullanÄ±cÄ± bazlÄ± veri iÅŸleme
â”œâ”€â”€ consumer_lake.py            # Bronze Layer Ingestion (Ham Veri Kaydedici)
â”œâ”€â”€ docker-compose.yaml         # TÃ¼m servislerin orkestrasyonu (17+ Servis)
â”œâ”€â”€ Dockerfile                  # Ana Python/Spark Uygulama Image TanÄ±mÄ±
â”œâ”€â”€ Dockerfile.connect          # Kafka Connect Ã–zelleÅŸtirilmiÅŸ Image
â”œâ”€â”€ Dockerfile.superset         # Apache Superset Ã–zelleÅŸtirilmiÅŸ Image
â”œâ”€â”€ fake_company.py             # Kurumsal Veri SimÃ¼latÃ¶rÃ¼ (HTTP POST)
â”œâ”€â”€ ingestion_api.py            # Universal API Gateway (FastAPI)
â”œâ”€â”€ ml_watcher.py               # Otonom Model EÄŸiticisi (Orchestrator)
â”œâ”€â”€ process_silver.py           # Silver Layer Processing & AI (Spark Streaming)
â”œâ”€â”€ producer.py                 # Binance WebSocket Veri Ãœreticisi
â”œâ”€â”€ prometheus.yml              # Prometheus Metrik KonfigÃ¼rasyonu
â”œâ”€â”€ quality_gate.py             #  Veri Kalite KontrolÃ¼ (Data Quality Gate)
â”œâ”€â”€ test_generic.py             # EndÃ¼striyel IoT SimÃ¼lasyonu
â”œâ”€â”€ train_model.py              # ML Model EÄŸitim Motoru (Spark MLlib)
â””â”€â”€ universal_producer.py       # Ã‡ok AmaÃ§lÄ± (Generic) Veri Ãœretici
```

##  Mimari TasarÄ±m (Architecture)

Sistem, verinin ham halden alÄ±narak anlamlÄ± iÅŸ zekasÄ± raporlarÄ±na dÃ¶nÃ¼ÅŸmesine kadar 5 ana katmandan oluÅŸur:

Proje, her biri belirli bir amaca hizmet eden modÃ¼ler bir yapÄ± Ã¼zerine inÅŸa edilmiÅŸtir. AÅŸaÄŸÄ±da, sistemin omurgasÄ±nÄ± oluÅŸturan dosyalarÄ±n detaylÄ± aÃ§Ä±klamalarÄ±nÄ± bulabilirsiniz:

####  Veri GiriÅŸi ve API (Ingestion)

-  **`producer.py` (Real-Time Ingestion Engine):** Binance WebSocket API'sine milisaniye hassasiyetinde baÄŸlanarak canlÄ± piyasa verilerini yakalayan ana veri sÃ¼rÃ¼cÃ¼sÃ¼dÃ¼r.
    
    -   **Asenkron MesajlaÅŸma:** Yakalanan trade verilerini Apache Kafka'nÄ±n `market_data` topic'ine asenkron olarak iletir.
        
    -   **GeliÅŸmiÅŸ Kafka YapÄ±landÄ±rmasÄ±:** * **Hata ToleransÄ±:** BaÄŸlantÄ± kopmalarÄ±na karÅŸÄ± `retries=5` yapÄ±landÄ±rmasÄ± ve `acks=1` (Leader Acknowledgement) ile hÄ±z-gÃ¼venlik dengesi optimize edilmiÅŸtir.
        
        -   **Performans:** AÄŸ bant geniÅŸliÄŸi tasarrufu iÃ§in `gzip` sÄ±kÄ±ÅŸtÄ±rma protokolÃ¼ kullanÄ±lmÄ±ÅŸtÄ±r.
            
    -   **Veri Normalizasyonu ve ZenginleÅŸtirme:** Binance'den gelen ham JSON verilerini (`s`, `p`, `q`, `T`) sistemle uyumlu `symbol`, `price`, `quantity` ve `timestamp` alanlarÄ±na haritalar (mapping).
        
    -   **Sistem Ä°zlenebilirliÄŸi:** Verinin Binance'den Ã§Ä±ktÄ±ÄŸÄ± an ile sisteme girdiÄŸi anÄ± karÅŸÄ±laÅŸtÄ±rmak iÃ§in `event_time` (ISO format) damgasÄ± ekleyerek gecikme (latency) analizine olanak tanÄ±r.
        
    -   **DayanÄ±klÄ±lÄ±k (Resilience):** WebSocket baÄŸlantÄ±sÄ±nÄ±n canlÄ± kalmasÄ± iÃ§in `ping_interval` kontrolÃ¼ ve Kafka broker hazÄ±r olana kadar devrede kalan `time.sleep(5)` tabanlÄ± dinamik yeniden baÄŸlanma (reconnection) mekanizmasÄ±na sahiptir.
    
### `ingestion_api.py`
> **Ã–nemli Not:** Bu modÃ¼l sistemin ana veri akÄ±ÅŸÄ±ndan baÄŸÄ±msÄ±z olarak tasarlanmÄ±ÅŸ bir **"opsiyonel geniÅŸletme katmanÄ±"**dÄ±r. Temel amacÄ± Binance dÄ±ÅŸÄ±ndaki Ã¶zel ÅŸirketlerin veya harici veri kaynaklarÄ±nÄ±n kendi verilerini sisteme dahil edebilmesi iÃ§in standart bir giriÅŸ kapÄ±sÄ± sunmaktÄ±r.

-   **`ingestion_api.py` (Universal API Gateway):** FastAPI tabanlÄ± asenkron bir uÃ§ nokta (endpoint) sunarak, dÄ±ÅŸ kaynaklardan gelen Ã¶zel finansal verileri Kafka ekosistemine dahil eden kÃ¶prÃ¼ modÃ¼lÃ¼dÃ¼r.
    
    -   **Esnek Veri KabulÃ¼:** Harici ÅŸirketlerin (`symbol`, `price`, `timestamp`) formatÄ±ndaki verilerini `POST` isteÄŸi ile kabul eder ve otomatik olarak `market_data` akÄ±ÅŸÄ±na enjekte eder.
        
    -   **Veri DoÄŸrulama ve Ã–n Ä°ÅŸleme:**
        
        -   Gelen isteklerde zorunlu alan kontrolÃ¼ yaparak hatalÄ± veri giriÅŸini (Bad Request) uygulama seviyesinde engeller.
            
        -   Eksik `quantity` verisi iÃ§eren isteklere otomatik olarak varsayÄ±lan deÄŸerler atayarak veri hattÄ±nÄ±n sÃ¼rekliliÄŸini korur.
            
    -   **Dinamik Kafka Entegrasyonu:** `get_kafka_producer` fonksiyonu ile Singleton tasarÄ±m desenine uygun ÅŸekilde tekil bir Kafka baÄŸlantÄ±sÄ± oluÅŸturur ve gelen verileri kuyruÄŸa (queue) asenkron olarak aktarÄ±r.
        
    -   **Hata YÃ¶netimi (Exception Handling):** Kafka baÄŸlantÄ± kopmalarÄ± veya geÃ§ersiz veri formatlarÄ± durumunda standart HTTP 500/400 hata kodlarÄ± ile istemciyi bilgilendirerek gÃ¼venli bir veri iletimi saÄŸlar.
    
-    **`fake_company.py`**: Sistemi test etmek iÃ§in geliÅŸtirilmiÅŸ bir simÃ¼latÃ¶rdÃ¼r. Kendi ÅŸirket verileriniz varmÄ±ÅŸ gibi FastAPI Ã¼zerinden sisteme veri gÃ¶nderir.
    

####  Veri Ä°ÅŸleme ve Storage (Processing & Lakehouse)

 **`process_silver.py` (The Heart of Analytics):** Apache Spark Structured Streaming mimarisini kullanarak Kafka'dan gelen ham verileri "Silver" katmanÄ±na dÃ¶nÃ¼ÅŸtÃ¼ren ve **"in-flight"** (akÄ±ÅŸ anÄ±nda) AI Ã§Ä±karÄ±mÄ± yapan modÃ¼ldÃ¼r.

-   **Hibrit Model YÃ¼kleme (Model Persistence):** `get_model_for_symbol` fonksiyonu ile MinIO (S3) Ã¼zerindeki en gÃ¼ncel regresyon modellerini (RandomForest, Linear, GBT, DecisionTree) dinamik olarak yÃ¼kler ve bellek yÃ¶netimi iÃ§in `model_cache` mekanizmasÄ±nÄ± kullanÄ±r.
    
-   **Mikro-YÄ±ÄŸÄ±n (Micro-batch) Stratejisi:** `.trigger(processingTime='5 seconds')` yapÄ±landÄ±rmasÄ±yla her 5 saniyede bir tetiklenen iÅŸlem dÃ¶ngÃ¼sÃ¼, dÃ¼ÅŸÃ¼k gecikmeli veri iÅŸleme ve veritabanÄ± senkronizasyonu saÄŸlar.
    
-   **GeliÅŸmiÅŸ Veri Normalizasyonu:** FarklÄ± kaynaklardan (Binance veya API Gateway) gelebilecek heterojen JSON verilerini `coalesce` fonksiyonu ile standartlaÅŸtÄ±rarak ÅŸema bÃ¼tÃ¼nlÃ¼ÄŸÃ¼nÃ¼ (Schema Enforcement) korur.
    
-   **7-Boyutlu Ã–znitelik MÃ¼hendisliÄŸi (Feature Vectorization):** Makine Ã¶ÄŸrenmesi modellerinin ihtiyaÃ§ duyduÄŸu Ã¶znitelikleri canlÄ± akÄ±ÅŸtan tÃ¼retir:
    
    -   **Ä°statistiksel Analiz:** 30 saniyelik pencerelerde `stddev_pop` ile anlÄ±k volatilite hesaplamasÄ± yapar.
        
    -   **VektÃ¶rleÅŸtirme:** `VectorAssembler` kullanarak volatilite, hareketli ortalamalar ve momentum gibi 7 farklÄ± deÄŸiÅŸkeni tek bir Ã¶zellik vektÃ¶rÃ¼nde birleÅŸtirir.
        
-   **Ã‡ok KatmanlÄ± Depolama (Polyglot Persistence):**
    
    -   **Lakehouse:** Analitik geÃ§miÅŸ ve ACID garantisi iÃ§in verileri Partitioned Delta Lake (S3a) formatÄ±nda arÅŸivler.
        
    -   **Operasyonel DB:** Dashboard sisteminin anlÄ±k beslenmesi iÃ§in PostgreSQL Ã¼zerine `foreachBatch` yÃ¶ntemiyle asenkron yazÄ±m gerÃ§ekleÅŸtirir.
        
-   **Hata ToleransÄ± (Resilience):** `checkpointLocation` kullanÄ±mÄ± sayesinde sistem kesintiye uÄŸrasa bile veri kaybÄ± yaÅŸamadan kaldÄ±ÄŸÄ± yerden devam edebilen bir yapÄ± sunar.
    

----------


-   **`consumer_lake.py` (The Data Archivist):** Apache Kafka'daki ham verileri (Raw Data) yakalayan ve ACID garantisi sunan **Delta Lake Bronze** katmanÄ±na kalÄ±cÄ± olarak kaydeden modÃ¼ldÃ¼r.
    
    -   **Spark-Delta Entegrasyonu:** Spark Session Ã¼zerinden Delta Lake uzantÄ±larÄ±nÄ± (`DeltaSparkSessionExtension`) aktif ederek, nesne depolama katmanÄ± (MinIO) Ã¼zerinde tam veri tutarlÄ±lÄ±ÄŸÄ± saÄŸlar.
        
    -   **GÃ¼venli Veri Edinimi (Reliable Streaming):**
        
        -   **`startingOffsets: earliest`**: Kafka topic'indeki tÃ¼m geÃ§miÅŸ verileri en baÅŸtan itibaren okuyarak veri kaybÄ±nÄ± Ã¶nler ve geÃ§miÅŸe dÃ¶nÃ¼k analiz (audit) imkanÄ± tanÄ±r.
            
        -   **`failOnDataLoss: false`**: Herhangi bir veri kaybÄ± durumunda sistemin durmasÄ±nÄ± engelleyerek sÃ¼rekliliÄŸi (fault-tolerance) saÄŸlar.
            
    -   **AkÄ±ÅŸ Optimizasyonu (Backpressure Management):** `maxOffsetsPerTrigger=1000` yapÄ±landÄ±rmasÄ± ile her mikro-yÄ±ÄŸÄ±nda maksimum 1000 mesaj iÅŸleyerek yÃ¼ksek trafik altÄ±nda sistemin tÄ±kanmasÄ±nÄ± (throttling) Ã¶nler.
        
    -   **YapÄ±sal DÃ¶nÃ¼ÅŸÃ¼m:** Kafka'dan binary formatta gelen verileri string'e cast ederek tanÄ±mlanmÄ±ÅŸ `StructType` ÅŸemasÄ±na gÃ¶re kolonlara ayÄ±rÄ±r.
        
    -   **Delta Lake Depolama Stratejisi:**
        
        -   **`partitionBy("symbol")`**: Verileri kripto paralarÄ±n sembollerine gÃ¶re fiziksel klasÃ¶rlere (partition) ayÄ±rarak, ilerideki sorgulama performansÄ±nÄ± optimize eder.
            
        -   **Checkpointing**: `checkpointLocation` kullanÄ±mÄ± ile yazma iÅŸlemi sÄ±rasÄ±nda oluÅŸabilecek kesintilerde Spark'Ä±n kaldÄ±ÄŸÄ± yerden devam etmesini saÄŸlar.
            
    -   **Mikro-YÄ±ÄŸÄ±n ZamanlamasÄ±:** `trigger(processingTime='10 seconds')` ile verileri 10 saniyelik aralÄ±klarla MinIO (S3a) Ã¼zerine `append` moduyla kalÄ±cÄ± olarak iÅŸler.
    
-    **`dbt_project/`**: Verinin Silver'dan Gold katmanÄ±na (Analitik katman) dÃ¶nÃ¼ÅŸÃ¼mÃ¼ iÃ§in gerekli SQL modellerini iÃ§erir. Veri temizleme ve aggregation iÅŸlemleri burada dÃ¶ner.
    

#### MLOps ve Otomasyon (Orchestration)


**`train_model.py` (The Intelligent Backbone):** Spark MLlib ve MLflow entegrasyonu ile Ã§alÄ±ÅŸan, Silver katmanÄ±ndaki verileri kullanarak en optimize tahmin modellerini otonom olarak Ã¼reten bir model geliÅŸtirme fabrikasÄ±dÄ±r.

-   **Zaman Serisi TabanlÄ± Ã–znitelik MÃ¼hendisliÄŸi (`create_smart_features`):** Ham fiyat verilerini finansal gÃ¶stergelere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r:
    
    -   **Lag (Gecikme) Analizi:** GeÃ§miÅŸ fiyat hareketlerini (`lag_1`, `lag_3`) modele girdi olarak sunar.
        
    -   **Hareketli Ortalamalar:** `ma_5` ve `ma_10` ile kÄ±sa ve orta vadeli trendleri hesaplar.
        
    -   **Dinamik GÃ¶stergeler:** Fiyat ivmesini (`momentum`) ve volatilite deÄŸiÅŸimlerini otonom olarak tÃ¼retir.
        
-   **Algoritma YarÄ±ÅŸmasÄ± (AutoML League):** Sistem, her eÄŸitim dÃ¶ngÃ¼sÃ¼nde dÃ¶rt farklÄ± gÃ¼Ã§lÃ¼ algoritmayÄ± birbiriyle yarÄ±ÅŸtÄ±rÄ±r:
    
    -   **ElasticNet (Linear Regression):** DÃ¼zenlileÅŸtirilmiÅŸ doÄŸrusal analiz.
        
    -   **DecisionTree & RandomForest:** Karar aÄŸacÄ± tabanlÄ± topluluk (ensemble) Ã¶ÄŸrenmesi.
        
    -   **GBTRegressor (Gradient Boosted Trees):** Hata odaklÄ± ardÄ±ÅŸÄ±k modelleme.
        
-   **MLOps ve Deney Takibi (MLflow):** Her eÄŸitim oturumu MLflow Ã¼zerinde kayÄ±t altÄ±na alÄ±nÄ±r; RMSE ve $R^2$ metrikleri, Ã¶znitelik Ã¶nem sÄ±ralamalarÄ± ve model parametreleri sistematik olarak loglanÄ±r.
    
-   **AkÄ±llÄ± Model SeÃ§imi ve DaÄŸÄ±tÄ±mÄ± (Champion Model):**
    
    -   **RMSE Optimizasyonu:** En dÃ¼ÅŸÃ¼k hata payÄ±na sahip model "KAZANAN" (Champion) olarak seÃ§ilir.
        
    -   **Production Deployment:** Kazanan model, `overwrite()` yÃ¶ntemiyle MinIO (S3) Ã¼zerindeki Ã¼retim yoluna otomatik olarak taÅŸÄ±nÄ±r ve `spark-silver` servisi tarafÄ±ndan canlÄ± tahminleme iÃ§in anÄ±nda kullanÄ±lmaya baÅŸlanÄ±r.
        
-   **Veri AyrÄ±ÅŸtÄ±rma Stratejisi:** Veriler rastgele deÄŸil, zaman serisi mantÄ±ÄŸÄ±na uygun olarak `%80` eÄŸitim ve `%20` test (son gelen veriler) ÅŸeklinde kronolojik olarak ayrÄ±ÅŸtÄ±rÄ±lÄ±r (`TimeSeries Split`)

 
PaylaÅŸtÄ±ÄŸÄ±n **`ml_watcher.py`** kodu, projenin **"Otonom Karar MekanizmasÄ±"**dÄ±r. Bu modÃ¼l, sistemin sÃ¼rekli baÅŸÄ±nda durmana gerek kalmadan, verinin olgunlaÅŸtÄ±ÄŸÄ±nÄ± anlar ve model eÄŸitim sÃ¼recini (`train_model.py`) akÄ±llÄ± bir ÅŸekilde tetikler.

Ä°ÅŸte bu kodun teknik iÅŸleyiÅŸini ve projedeki stratejik Ã¶nemini anlatan profesyonel aÃ§Ä±klama metni:

----------

### â³ Otonom Model Ä°zleme ve Tetikleme Sistemi (ML Watcher)

-   **`ml_watcher.py` (The Orchestration Sentry):** Delta Lake Ã¼zerindeki veri hacmini sÃ¼rekli denetleyen ve sistemin "kendi kendini eÄŸitme" (Self-training) kabiliyetini yÃ¶neten bekÃ§i modÃ¼lÃ¼dÃ¼r.
    
    -   **Olay GÃ¼dÃ¼mlÃ¼ EÄŸitim (Event-Driven Training):** Sabit bir zaman Ã§izelgesi yerine, veri odaklÄ± bir strateji izler:
        
        -   **AvcÄ± Modu (Initial Hunt):** Sistem ilk baÅŸladÄ±ÄŸÄ±nda, Silver katmanÄ±nda `MIN_ROWS_TO_START=20` eÅŸiÄŸine ulaÅŸÄ±lana kadar 10 saniyede bir tarama yaparak ilk modelin en kÄ±sa sÃ¼rede Ã¼retilmesini saÄŸlar.
            
        -   **Devriye Modu (Maintenance):** Ä°lk eÄŸitim tamamlandÄ±ktan sonra, sistem kaynaklarÄ±nÄ± korumak amacÄ±yla 5 dakikalÄ±k (`NORMAL_INTERVAL_SEC`) periyotlarla dÃ¼zenli kontrollere geÃ§er.
            
    -   **DoÄŸrudan Delta Lake Entegrasyonu:** `deltalake` kÃ¼tÃ¼phanesi ve `storage_options` Ã¼zerinden MinIO (S3) ile doÄŸrudan konuÅŸarak Spark'a ihtiyaÃ§ duymadan veri sayÄ±mÄ± (row count) yapar; bu sayede dÃ¼ÅŸÃ¼k kaynak tÃ¼ketimiyle izleme gerÃ§ekleÅŸtirir.
        
    -   **Alt SÃ¼reÃ§ YÃ¶netimi (Subprocess Orchestration):** `subprocess.run` mekanizmasÄ± ile `train_model.py` dosyasÄ±nÄ± baÄŸÄ±msÄ±z bir iÅŸlem olarak baÅŸlatÄ±r, Ã§Ä±ktÄ±larÄ± (stdout/stderr) yakalayarak eÄŸitim baÅŸarÄ±sÄ±nÄ± doÄŸrular.
        
    -   **Hata YÃ¶netimi ve DayanÄ±klÄ±lÄ±k:** HenÃ¼z veri oluÅŸmamÄ±ÅŸ olmasÄ± veya aÄŸ gecikmeleri gibi istisnai durumlarÄ± `try-except` bloklarÄ± ile yÃ¶neterek izleme sÃ¼recinin kesintisiz devam etmesini saÄŸlar.
    
#### ğŸ–¥ï¸ ArayÃ¼z ve AltyapÄ± (UI & DevOps)

-   ğŸ“Š **`dashboard.py`**: **Streamlit** ile geliÅŸtirilmiÅŸ komuta merkezidir. CanlÄ± fiyat akÄ±ÅŸÄ±nÄ±, yapay zeka tahminlerini ve sistem saÄŸlÄ±ÄŸÄ±nÄ± gÃ¶rselleÅŸtirir.
    
-   ğŸ³ **`docker-compose.yaml`**: TÃ¼m ekosistemi (Kafka, Spark, Airflow, MinIO, Postgres vb.) birbirine baÄŸlÄ± ve izole bir ÅŸekilde ayaÄŸa kaldÄ±ran ana orkestrasyon dosyasÄ±dÄ±r.TÃ¼m ekosistemi (17+ servis) izole ve baÄŸÄ±mlÄ±lÄ±klarÄ± yÃ¶netilmiÅŸ ÅŸekilde ayaÄŸa kaldÄ±rÄ±r.
    
-   ğŸ“¦ **`Dockerfile` / `Dockerfile.spark`**: Spark ve API gibi Ã¶zel servislerin Ã§alÄ±ÅŸmasÄ± iÃ§in gerekli kÃ¼tÃ¼phane ve baÄŸÄ±mlÄ±lÄ±klarÄ±n (Python, Java, Delta Jar) tanÄ±mlandÄ±ÄŸÄ± paketleme dosyalarÄ±dÄ±r.
    
-   ğŸ“‘ **`requirements.txt`**: Projenin Ã§alÄ±ÅŸmasÄ± iÃ§in gerekli tÃ¼m Python kÃ¼tÃ¼phanelerinin (PySpark, Kafka-Python, Delta-Spark, FastAPI) listesidir.
----------
## ğŸ› ï¸ Kurulum ve Ã‡alÄ±ÅŸtÄ±rma Rehberi
### 1. Sistemi BaÅŸlatma

Docker konteynerlerini (Kafka, Spark, Airflow, Postgres, MinIO vb.) derler ve arka planda Ã§alÄ±ÅŸtÄ±rÄ±r:
Bash
```
Ä°lk Ã¶nce .env ile dosya olusturun.
.env
```
```
docker-compose up -d --build
```
### 2. AI Modellerini EÄŸitme

Sistemde yeterli veri biriktikten sonra modelleri eÄŸitmek ve MLflow'a kaydetmek iÃ§in:
Bash
```
# Checkpoint silmeden eÄŸitim baÅŸlatmak
1.  Manuel olarak:
docker exec -it spark-silver python train_model.py
2.  Otomatik olarak:
docker exec -it spark-silver python ml_watcher.py
# Checkpoint silerek eÄŸitimi sÄ±fÄ±rdan baslatmak
docker exec -it spark-silver rm -rf /app/checkpoints_silver_1
```

### 3. dbt DÃ¶nÃ¼ÅŸÃ¼mlerini Ã‡alÄ±ÅŸtÄ±rma
Verileri PostgreSQL Gold katmanÄ±na dÃ¶nÃ¼ÅŸtÃ¼rmek ve analitik hazÄ±rlÄ±k yapmak iÃ§in:
Bash
```
docker exec dbt_transformer dbt run
```

----------

## ğŸ“Š Ä°zleme ve Analiz Panelleri

**Servis Port KullanÄ±m AmacÄ±**

**Streamlit Dashboard**:
`http://localhost:8501` CanlÄ± Teknik Analiz & AI Tahmin BandÄ±.

**Metabase BI**
`http://localhost:3005/`Kurumsal SQL Raporlama & Business Intelligence.

**MLflow**
`http://localhost:5000/`Model Versiyonlama ve Performans Takibi.

**KrafDrop**
`http://localhost:9010`BÄ±nance veri akÄ±sÄ±nÄ± izleme.

**MinIO Console**
`http://localhost:9001`S3 Lakehouse Veri GÃ¶rÃ¼ntÃ¼leyici.

**Grafana**
`http://localhost:3001/`Sistem SaÄŸlÄ±ÄŸÄ± ve AltyapÄ± Ä°zleme.

**CAdvisor**
`http://localhost:8090/containers/`

**API Docs**
`http://localhost:8000/docs`FastAPI Swagger DokÃ¼mantasyonu.

----------

## ğŸ‘¨â€ğŸ’» GeliÅŸtirici NotlarÄ± (Ops & Debug)

### **Kodlarda deÄŸiÅŸiklik yaptÄ±ÄŸÄ±nda tÃ¼m sistemi kapatÄ±p aÃ§mana gerek yok.**

Ã¶rnek:Konteyneri durdurmadan dashboard kodunu gÃ¼ncellemek iÃ§in:
Bash
```
docker cp dashboard.py dashboard:/app/dashboard.py
docker restart dashboard
```
### **KÃ¶klÃ¼ DeÄŸiÅŸiklik veya KÃ¼tÃ¼phane Eklediysen (Dockerfile).**
Bash
```
docker-compose up -d --build
```

### **Veri DoÄŸrulama (SQL)**

Verilerin doÄŸru yazÄ±ldÄ±ÄŸÄ±nÄ± PostgreSQL iÃ§inden kontrol etmek iÃ§in:
Bash
```
docker exec -it postgres psql -U admin -d market_db -c "SELECT * FROM crypto_prices LIMIT 10;"
```

### **Eklenen yeni Ã¶zellikler v1**
**[TR]** Proje, baÅŸlangÄ±Ã§ta sadece Binance API'sine baÄŸlÄ±yken, yapÄ±lan bÃ¼yÃ¼k mimari gÃ¼ncellemelerle **Veri BaÄŸÄ±msÄ±z (Data Agnostic)** bir yapÄ±ya dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸtÃ¼r. ArtÄ±k sistem, veri kaynaÄŸÄ±nÄ±n ne olduÄŸuna bakmaksÄ±zÄ±n (IoT, Finans, E-Ticaret, Loglar) her tÃ¼rlÃ¼ JSON verisini dinamik olarak iÅŸleyip MinIO Data Lake'e arÅŸivleyebilir.
### ğŸš€ Key Capabilities (Neler Eklendi?)

#### 1. ğŸ§¬ Generic Producer & Schema Agnostic Ingestion

Eski "Hardcoded" (sabit kodlu) yapÄ± terk edildi. Producer ve Kafka katmanÄ± artÄ±k verinin iÃ§eriÄŸiyle ilgilenmez, sadece taÅŸÄ±yÄ±cÄ± gÃ¶revi gÃ¶rÃ¼r.

-   **Ã–nceden:** Sadece `symbol`, `price` sÃ¼tunlarÄ±nÄ± kabul ediyordu.
    
-   **Åimdi:** Herhangi bir JSON yapÄ±sÄ±nÄ± kabul eder. Sistem `data_type` etiketine gÃ¶re veriyi otomatik tanÄ±r.
    

#### 2. âš¡ Dynamic Spark Processing (Polymorphic Handling)

Apache Spark Streaming job'larÄ±, gelen veriyi dinamik olarak ayrÄ±ÅŸtÄ±rÄ±r (parsing).

-   Gelen veri **CRYPTO** ise -> Finansal analiz ÅŸemasÄ±na sokar.
    
-   Gelen veri **IOT** ise -> SensÃ¶r/Telemetri ÅŸemasÄ±na sokar.
    
-   Gelen veri **LOGS** ise -> Metin analizi ÅŸemasÄ±na sokar.
    

#### 3. ğŸ—„ï¸ Multi-Tenant Data Lake Storage (MinIO)

Veriler MinIO Ã¼zerinde karmaÅŸÄ±k halde deÄŸil, kaynaklarÄ±na gÃ¶re otomatik bÃ¶lÃ¼mlenmiÅŸ (partitioned) klasÃ¶rlerde saklanÄ±r.

-   `s3a://lakehouse/data/source=binance/...`
    
-   `s3a://lakehouse/data/source=iot_sensors/...`
    
-   `s3a://lakehouse/data/source=stock_market/...`
### ğŸ’» Code Example: How It Handles Different Data

Sistemin esnekliÄŸini gÃ¶steren JSON yapÄ±larÄ±:

JSON

```
// Gelen Veri 1: Kripto (Sistem bunu otomatik tanÄ±r)
{
  "source": "BINANCE",
  "payload": { "symbol": "BTC", "price": 45000, "vol": 120 }
}

// Gelen Veri 2: Fabrika SensÃ¶rÃ¼ (Kod deÄŸiÅŸtirmeden bunu da iÅŸler)
{
  "source": "IOT_SENSOR",
  "payload": { "device_id": "XJ-9", "temp": 85.5, "vibration": "HIGH" }
}
```
###ğŸ›¡ï¸ Data Quality Gate (Veri Kalite ve SaÄŸlÄ±k KapÄ±sÄ±)
Sistemin kÃ¶rÃ¼ kÃ¶rÃ¼ne veri iÅŸlemesini engelleyen, **Lakehouse Mimarisinin "Doktoru"** olarak gÃ¶rev yapan denetim katmanÄ±dÄ±r. Silver (Ä°ÅŸlenmiÅŸ) katmandaki verilerin finansal ve teknik tutarlÄ±lÄ±ÄŸÄ±nÄ± analiz eder.

BÃ¼yÃ¼k veri projelerindeki *"Garbage In, Garbage Out"* (Ã‡Ã¶p girerse Ã§Ã¶p Ã§Ä±kar) problemini engellemek iÃ§in geliÅŸtirilmiÅŸtir.

### ğŸ” Denetim MekanizmasÄ± (Audit Logic)
**`quality_gate.py`** modÃ¼lÃ¼, Spark motorunu kullanarak Delta Lake Ã¼zerinde ÅŸu kritik kontrolleri gerÃ§ekleÅŸtirir:

1.  **ğŸ“‰ Negatif Fiyat KontrolÃ¼ (Financial Integrity):**
    * Finansal piyasalarda varlÄ±k fiyatÄ± `0` veya negatif olamaz. Bu durum veri kaynaÄŸÄ±ndaki bir hatayÄ± (bug) iÅŸaret eder.
2.  **ğŸ§© Eksik Veri TaramasÄ± (Null Safety):**
    * Yapay zeka modellerinin beslendiÄŸi `volatility` (oynaklÄ±k) veya teknik indikatÃ¶rlerin hesaplanÄ±p hesaplanmadÄ±ÄŸÄ±nÄ± kontrol eder.
3.  **â³ Zaman TutarlÄ±lÄ±ÄŸÄ± (Temporal Validity):**
    * `processed_time` veya `event_time` alanlarÄ±nda bozulma olup olmadÄ±ÄŸÄ±nÄ±, gelecekten gelen (Future Date) hatalÄ± verileri denetler.

### ğŸ“Š Raporlama ve Aksiyon
Sistem tarama sonucunda iki tÃ¼r rapor Ã¼retir:
* âœ… **PASSED (MÃ¼kemmel):** Veri seti temiz, ML modelleri eÄŸitilebilir.
* âŒ **FAILED (HatalÄ±):** Kritik hatalar tespit edildi, ilgili satÄ±rlar karantinaya alÄ±nmalÄ± veya alarm Ã¼retilmeli.

**NasÄ±l Ã‡alÄ±ÅŸtÄ±rÄ±lÄ±r?**
Bu denetim, **Enterprise Control Plane (Sistem YÃ¶netimi Paneli)** Ã¼zerinden tek tÄ±kla canlÄ± log takibiyle Ã§alÄ±ÅŸtÄ±rÄ±labildiÄŸi gibi, terminal Ã¼zerinden de tetiklenebilir:

```bash
docker exec -it spark-silver python quality_gate.py
```    

## ğŸ¤ KatkÄ±da Bulunun (Contributing)

Bu proje bir **YBS Ã¶ÄŸrencisi** tarafÄ±ndan geliÅŸtirilmiÅŸ aÃ§Ä±k kaynaklÄ± bir framework'tÃ¼r. Her tÃ¼rlÃ¼ katkÄ±ya, fikre ve PR'a aÃ§Ä±ktÄ±r.

-   **GeliÅŸtirici:** Ã–mer Ã‡akan
    
-   **LinkedIn:** www.linkedin.com/in/Ã¶mer-Ã§akan-819751261
    
-   **Destek:** Proje size yardÄ±mcÄ± olduysa bir â­ bÄ±rakmayÄ± unutmayÄ±n!
- ### 3. KatÄ±lÄ±mcÄ±lara Ã–zel Kod TalimatÄ±

Kendi branch'inizi aÃ§Ä±n, ama benim `main`'ime dokunmayÄ±n."

Bash
```
# 1. Ã–nce projeyi kendi bilgisyarÄ±na indir ya da dÄ±rekt 
github Ã¼zerinden indir
git clone https://github.com/propaper12/An-Open-Source-Real-Time-Financial-Lakehouse-Project.git
# 2. Kendi adÄ±nÄ±za veya Ã¶zelliÄŸinize gÃ¶re yeni bir branch aÃ§Ä±n
git checkout -b dev/herhangi_isim
# 3. GeliÅŸtirmenizi yapÄ±n ve sadece bu branch'e pushlayÄ±n
git push origin dev/herhangi_isim
```
## ğŸ¤ Projenin gÃ¶rselleri:
<img width="1530" height="654" alt="Ekran gÃ¶rÃ¼ntÃ¼sÃ¼ 2026-02-05 174921" src="https://github.com/user-attachments/assets/d5e0de38-6b3d-4caf-aff0-bcbfeb7d27c6" />
<img width="2790" height="1415" alt="Ekran gÃ¶rÃ¼ntÃ¼sÃ¼ 2026-02-05 171703" src="https://github.com/user-attachments/assets/f86b504f-9564-41b1-8b5a-86956aba1515" />
<img width="2563" height="1467" alt="Ekran gÃ¶rÃ¼ntÃ¼sÃ¼ 2026-02-05 171629" src="https://github.com/user-attachments/assets/100441f6-084d-4f43-9fef-88006e93f122" />
<img width="2560" height="1457" alt="Ekran gÃ¶rÃ¼ntÃ¼sÃ¼ 2026-02-05 171614" src="https://github.com/user-attachments/assets/e2db796b-3dcd-452e-96b8-07e9a54289c3" />
<img width="2785" height="1454" alt="Ekran gÃ¶rÃ¼ntÃ¼sÃ¼ 2026-02-05 171552" src="https://github.com/user-attachments/assets/c4658139-de33-40a2-a99d-4a7210ae44f1" />
<img width="1095" height="730" alt="Ekran gÃ¶rÃ¼ntÃ¼sÃ¼ 2026-02-05 171448" src="https://github.com/user-attachments/assets/ab8210ca-9b1c-471c-a487-fc46b80bf481" />
<img width="1081" height="1280" alt="Ekran gÃ¶rÃ¼ntÃ¼sÃ¼ 2026-02-05 171440" src="https://github.com/user-attachments/assets/1c3657d4-c6c0-404f-af50-fd1f2c28c2fc" />
<img width="2793" height="1455" alt="Ekran gÃ¶rÃ¼ntÃ¼sÃ¼ 2026-02-05 171227" src="https://github.com/user-attachments/assets/22a9d585-84bc-424f-a320-424fc3e17227" />
<img width="2772" height="1476" alt="Ekran gÃ¶rÃ¼ntÃ¼sÃ¼ 2026-02-05 170637" src="https://github.com/user-attachments/assets/6548da13-a35f-4d57-ac58-c02da3c0969e" />
